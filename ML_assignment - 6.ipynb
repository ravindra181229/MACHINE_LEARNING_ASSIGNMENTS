{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35fb87a",
   "metadata": {},
   "source": [
    "# ML_assignment - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b797e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA Model :\\nA Machine learning model is a file that has been trained to recognize certain types of patterns. You train a model over a set of data, providing it an algorithm that it can use to reason over and learn from those data\\n\\nThe best way to train a model:\\n>>Begin with existing data. Machine learning requires us to have existing data—not the data our application will use when we run it, but data to learn from\\n>>Analyze data to identify patterns\\n>>Make predictions\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1\n",
    "\"\"\"\n",
    "A Model :\n",
    "A Machine learning model is a file that has been trained to recognize certain types of patterns. You train a model over a set of data, providing it an algorithm that it can use to reason over and learn from those data\n",
    "\n",
    "The best way to train a model:\n",
    ">>Begin with existing data. Machine learning requires us to have existing data—not the data our application will use when we run it, but data to learn from\n",
    ">>Analyze data to identify patterns\n",
    ">>Make predictions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "624c2924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe No Free Lunch Theorem often known as NFL or NFLT, is a theoretical finding that suggests all optimization algorithms \\nperform equally well when their performance is averaged over all possible objective functions\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2\n",
    "\"\"\"\n",
    "The No Free Lunch Theorem often known as NFL or NFLT, is a theoretical finding that suggests all optimization algorithms \n",
    "perform equally well when their performance is averaged over all possible objective functions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6c2ea6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples.\\nOf the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3\n",
    "\"\"\"\n",
    "In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples.\n",
    "Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "647647a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBootstrap sampling is used in a machine learning ensemble algorithm called bootstrap aggregating (also called bagging). \\nIt helps in avoiding overfitting and improves the stability of machine learning algorithms. In bagging, a certain number of equally sized subsets of a dataset are extracted with replacement\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q4\n",
    "\"\"\"\n",
    "Bootstrap sampling is used in a machine learning ensemble algorithm called bootstrap aggregating (also called bagging). \n",
    "It helps in avoiding overfitting and improves the stability of machine learning algorithms. In bagging, a certain number of equally sized subsets of a dataset are extracted with replacement\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40cffef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n>> It basically tells you how much better your classifier is performing over the performance of a classifierthat simply guesses at random according to the frequency of each class.\\nCohen's kappa is always less than or equal to 1. Values of 0 or less, indicate that the classifier is useless\\n\\n>>To calculate Observed Accuracy, we simply add the number of instances that the machine learning \\nclassifier agreed with the ground truth label, and divide by the total number of instances. For this confusion matrix, this would be 0.6 ((10 + 8) / 30 = 0.6).\\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q5\n",
    "\"\"\"\n",
    ">> It basically tells you how much better your classifier is performing over the performance of a classifierthat simply guesses at random according to the frequency of each class.\n",
    "Cohen's kappa is always less than or equal to 1. Values of 0 or less, indicate that the classifier is useless\n",
    "\n",
    ">>To calculate Observed Accuracy, we simply add the number of instances that the machine learning \n",
    "classifier agreed with the ground truth label, and divide by the total number of instances. For this confusion matrix, this would be 0.6 ((10 + 8) / 30 = 0.6).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e33e1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEnsemble methods are techniques that create multiple models and then combine them \\nto produce improved results. Ensemble methods usually produces more accurate solutions than a single model would. \\nThis has been the case in a number of machine learning competitions, where the winning solutions used ensemble methods.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q6\n",
    "\"\"\"\n",
    "Ensemble methods are techniques that create multiple models and then combine them \n",
    "to produce improved results. Ensemble methods usually produces more accurate solutions than a single model would. \n",
    "This has been the case in a number of machine learning competitions, where the winning solutions used ensemble methods.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9badc3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA descriptive model describes a system or other entity and its relationship to its environment.\\nIt is generally used to help specify and/or understand what the system is, what it does, and how it does it. A geometric model or spatial model is a descriptive model that represents geometric and/or spatial relationships.\\n\\nExamples:\\nClimate Change,\\nHealth Care,\\nFood Insecurity,\\nViolence,\\nHomelessness,\\nSustainability,\\nEducation.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q7\n",
    "\"\"\"\n",
    "A descriptive model describes a system or other entity and its relationship to its environment.\n",
    "It is generally used to help specify and/or understand what the system is, what it does, and how it does it. A geometric model or spatial model is a descriptive model that represents geometric and/or spatial relationships.\n",
    "\n",
    "Examples:\n",
    "Climate Change,\n",
    "Health Care,\n",
    "Food Insecurity,\n",
    "Violence,\n",
    "Homelessness,\n",
    "Sustainability,\n",
    "Education.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1eb60844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThere are 3 main metrics for model evaluation in regression:\\nR Square/Adjusted R Square.\\nMean Square Error(MSE)/Root Mean Square Error(RMSE)\\nMean Absolute Error(MAE)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q8\n",
    "\"\"\"\n",
    "There are 3 main metrics for model evaluation in regression:\n",
    "R Square/Adjusted R Square.\n",
    "Mean Square Error(MSE)/Root Mean Square Error(RMSE)\n",
    "Mean Absolute Error(MAE)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "708d0ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Descriptive vs. predictive models:\\n>> A descriptive model will exploit the past data that are stored in databases and provide you with the accurate report.\\n\\n>>  In a Predictive model, it identifies patterns found in past and transactional data to find risks and future outcomes.\\n\\n2.Underfitting vs. overfitting the model:\\n>> Underfitting means that your model makes accurate, but initially incorrect predictions. In this case, train error is large and val/test error is large too. \\n\\n>> Overfitting means that your model makes not accurate predictions. In this case, train error is very small and val/test error is large\\n\\n3.Bootstrapping vs. cross-validation:\\n>> Cross validation splits the available dataset to create multiple datasets\\n\\n>> Bootstrapping method uses the original dataset to create multiple datasets after resampling with replacement\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q9\n",
    "\"\"\"\n",
    "1. Descriptive vs. predictive models:\n",
    ">> A descriptive model will exploit the past data that are stored in databases and provide you with the accurate report.\n",
    "\n",
    ">>  In a Predictive model, it identifies patterns found in past and transactional data to find risks and future outcomes.\n",
    "\n",
    "2.Underfitting vs. overfitting the model:\n",
    ">> Underfitting means that your model makes accurate, but initially incorrect predictions. In this case, train error is large and val/test error is large too. \n",
    "\n",
    ">> Overfitting means that your model makes not accurate predictions. In this case, train error is very small and val/test error is large\n",
    "\n",
    "3.Bootstrapping vs. cross-validation:\n",
    ">> Cross validation splits the available dataset to create multiple datasets\n",
    "\n",
    ">> Bootstrapping method uses the original dataset to create multiple datasets after resampling with replacement\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11b01000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1.LOOCV:\\n>> LOOCV(Leave One Out Cross-Validation) is a type of cross-validation approach in which each observation is considered as the validation set\\nand the rest (N-1) observations are considered as the training set. In LOOCV, fitting of the model is done and predicting using one observation validation set\\n\\n2.F-measurement:\\n>> The F measure (F1 score or F score) is a measure of a test's accuracy and is defined as the weighted harmonic mean of the precision and recall of the test.\\n\\n3.The width of the silhouette\\n>> The Average Silhouette Width (ASW) of a clustering is ̄ a ( i ) is the average distance of to points in the cluster to which it was assigned, \\nand is the average distance of to the points in the nearest cluster to which it was not assigned.\\n\\n4.Receiver operating characteristic curve:\\n>> The ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 – FPR). \\nClassifiers that give curves closer to the top-left corner indicate a better performance. As a baseline, a random classifier is expected to give points lying along the diagonal (FPR = TPR)\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q10\n",
    "\"\"\"\n",
    "1.LOOCV:\n",
    ">> LOOCV(Leave One Out Cross-Validation) is a type of cross-validation approach in which each observation is considered as the validation set\n",
    "and the rest (N-1) observations are considered as the training set. In LOOCV, fitting of the model is done and predicting using one observation validation set\n",
    "\n",
    "2.F-measurement:\n",
    ">> The F measure (F1 score or F score) is a measure of a test's accuracy and is defined as the weighted harmonic mean of the precision and recall of the test.\n",
    "\n",
    "3.The width of the silhouette\n",
    ">> The Average Silhouette Width (ASW) of a clustering is ̄ a ( i ) is the average distance of to points in the cluster to which it was assigned, \n",
    "and is the average distance of to the points in the nearest cluster to which it was not assigned.\n",
    "\n",
    "4.Receiver operating characteristic curve:\n",
    ">> The ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 – FPR). \n",
    "Classifiers that give curves closer to the top-left corner indicate a better performance. As a baseline, a random classifier is expected to give points lying along the diagonal (FPR = TPR)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de004f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

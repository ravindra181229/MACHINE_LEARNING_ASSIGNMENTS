{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "636d89c8",
   "metadata": {},
   "source": [
    "# ML_assignement - 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a9e188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon.\\nChoosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition, classification and regression.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1\n",
    "\"\"\"\n",
    "In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon.\n",
    "Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition, classification and regression.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a16cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe process of generating new variables based on already existing variables is known as feature construction.\\nFeature Construction is a useful process as it can add more information and give more insights of the data we are dealing with.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2\n",
    "\"\"\"\n",
    "The process of generating new variables based on already existing variables is known as feature construction.\n",
    "Feature Construction is a useful process as it can add more information and give more insights of the data we are dealing with.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9376286d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWhen we have a feature where variables are just names and there is no order or rank to this variable's feature.\\nExample: City of person lives in, Gender of person,\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3\n",
    "\"\"\"\n",
    "When we have a feature where variables are just names and there is no order or rank to this variable's feature.\n",
    "Example: City of person lives in, Gender of person,\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8302ac35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nconverting numeric data to categorical data seems like an easy problem. \\nOne simple approach would be to divide the raw source data into equal intervals.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q4\n",
    "\"\"\"\n",
    "converting numeric data to categorical data seems like an easy problem. \n",
    "One simple approach would be to divide the raw source data into equal intervals.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b544bef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn wrapper methods, the feature selection process is based on a specific machine learning algorithm that we are trying to fit on a given dataset.\\nAdvantages :\\nThe wrapper classification algorithms with joint dimensionality reduction and classification can also be used but these methods have high computation cost, lower discriminative power.\\n\\nDisadvantages : \\nThere are several disadvantages with WFS such as computationally intensive, discriminative power, lower shorter training times, classifier dependent selection,\\nand higher risk of over-fitting than deterministic algorithms\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q5\n",
    "\"\"\"\n",
    "In wrapper methods, the feature selection process is based on a specific machine learning algorithm that we are trying to fit on a given dataset.\n",
    "Advantages :\n",
    "The wrapper classification algorithms with joint dimensionality reduction and classification can also be used but these methods have high computation cost, lower discriminative power.\n",
    "\n",
    "Disadvantages : \n",
    "There are several disadvantages with WFS such as computationally intensive, discriminative power, lower shorter training times, classifier dependent selection,\n",
    "and higher risk of over-fitting than deterministic algorithms\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fddba83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA feature may be deemed irrelevant if it has poor data quality.\\nWe should be concerned whenever we expect recorded values in our dataset are different from true values.\\n\\n>> To quantify we can use some methods :\\nWrapper methods (forward, backward, and stepwise selection), \\nFilter methods (ANOVA, Pearson correlation, variance thresholding),\\nand Embedded methods (Lasso, Ridge, Decision Tree)\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q6\n",
    "\"\"\"\n",
    "A feature may be deemed irrelevant if it has poor data quality.\n",
    "We should be concerned whenever we expect recorded values in our dataset are different from true values.\n",
    "\n",
    ">> To quantify we can use some methods :\n",
    "Wrapper methods (forward, backward, and stepwise selection), \n",
    "Filter methods (ANOVA, Pearson correlation, variance thresholding),\n",
    "and Embedded methods (Lasso, Ridge, Decision Tree)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3756b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n>> Duplicated features or information, that adds as a precaution against failure or error is known as Feature redundancy.\\n>>if two features {X1, X2} are highly correlated, then the two features become redundant features since they have same information in terms of correlation measure.\\nIn other words, the correlation measure provides statistical association between any given a pair of features\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q7\n",
    "\"\"\"\n",
    ">> Duplicated features or information, that adds as a precaution against failure or error is known as Feature redundancy.\n",
    ">>if two features {X1, X2} are highly correlated, then the two features become redundant features since they have same information in terms of correlation measure.\n",
    "In other words, the correlation measure provides statistical association between any given a pair of features\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab7e545e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHamming distance, Euclidean distance , Manhattan Distance\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q8\n",
    "\"\"\"\n",
    "Hamming distance, Euclidean distance , Manhattan Distance\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4231ab6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEuclidean distance is the shortest path between source and destination which is a straight line.\\n>> Manhattan distance is sum of all the real distances between source(s) and destination(d) \\nand each distance are always the straight lines\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q9\n",
    "\"\"\"\n",
    "Euclidean distance is the shortest path between source and destination which is a straight line.\n",
    ">> Manhattan distance is sum of all the real distances between source(s) and destination(d) \n",
    "and each distance are always the straight lines\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9280d677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfeature transformation: transformation of data to improve the accuracy of the algorithm\\nfeature selection: removing unnecessary features\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q10\n",
    "\"\"\"\n",
    "feature transformation: transformation of data to improve the accuracy of the algorithm\n",
    "feature selection: removing unnecessary features\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6dd0cb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1.SVD (Standard Variable Diameter Diameter) : \\n>> we achieve dimensionality reduction using SVD, We can use the first k columns of V and S and achieve U' with fewer columns. \\nThis reduced U can now be used as a proxy for matrix dat with fewer columns\\n\\n2.Collection of features using a hybrid approach :\\n>> A hybrid feature selection method is proposed for classification in small sample size data sets.\\nThe filter step is based on instance learning taking advantage of the small sample size of data \\nA few candidate feature subsets are generated since their number corresponds to the number of instances\\n\\n3.The width of the silhouette : \\n>> The Average Silhouette Width (ASW) of a clustering is ̄ a ( i ) is the average distance of to points in the cluster to which it was assigned\\nand is the average distance of to the points in the nearest cluster to which it was not assigned.\\n\\n4.Receiver operating characteristic curve :\\n>> The ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 – FPR). \\nClassifiers that give curves closer to the top-left corner indicate a better performance\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q11\n",
    "\"\"\"\n",
    "1.SVD (Standard Variable Diameter Diameter) : \n",
    ">> we achieve dimensionality reduction using SVD, We can use the first k columns of V and S and achieve U' with fewer columns. \n",
    "This reduced U can now be used as a proxy for matrix dat with fewer columns\n",
    "\n",
    "2.Collection of features using a hybrid approach :\n",
    ">> A hybrid feature selection method is proposed for classification in small sample size data sets.\n",
    "The filter step is based on instance learning taking advantage of the small sample size of data \n",
    "A few candidate feature subsets are generated since their number corresponds to the number of instances\n",
    "\n",
    "3.The width of the silhouette : \n",
    ">> The Average Silhouette Width (ASW) of a clustering is ̄ a ( i ) is the average distance of to points in the cluster to which it was assigned\n",
    "and is the average distance of to the points in the nearest cluster to which it was not assigned.\n",
    "\n",
    "4.Receiver operating characteristic curve :\n",
    ">> The ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 – FPR). \n",
    "Classifiers that give curves closer to the top-left corner indicate a better performance\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0572b71e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
